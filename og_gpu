import random

import numpy as np
import pandas as pd
import networkx as nx
from itertools import product
from collections import defaultdict


class HIN:
    """
    Class to generate vertex sequences.
    """

    def __init__(self, window=None):
        self.graph = nx.DiGraph()
        self.node_size = 0
        self._path_size = 0

        def new_id():
            i = self.node_size
            self.node_size += 1
            return i

        self._node2id = defaultdict(new_id)
        self._id2type = {}
        self._window = window
        self._node_types = set()
        self._path2id = None
        self._id2path = None
        self._id2node = None

    @property
    def id2node(self):
        return self._id2node

    @property
    def id2path(self):
        return self._id2path

    @property
    def window(self):
        return self._window

    @window.setter
    def window(self, val):
        if not self._window:
            self._window = val
        else:
            raise ValueError("window只能被设定一次")

    @property
    def path_size(self):
        if not self._path_size:
            raise ValueError("run sample() first to count path size")
        return self._path_size

    def add_edge(self, source_node, source_class, dest_node, dest_class, edge_class, weight):
        i = self._node2id[source_node]
        j = self._node2id[dest_node]
        self._id2type[i] = source_class
        self._id2type[j] = dest_class
        self._node_types.add(source_class)
        self._node_types.add(dest_class)
        self.graph.add_edge(i, j, weight=weight)

    def small_walk(self, start_node, length):
        walk = [start_node]
        for i in range(1, length):
            if next(iter(nx.neighbors(self.graph, walk[-1])), None) is None:
                break
            cur_node = walk[-1]
            nodes = list(nx.neighbors(self.graph, cur_node))
            weights = [self.graph[cur_node][i]['weight'] for i in nodes]  # 有向图可能不能这么做
            s = sum(weights)
            weights = [i/s for i in weights]
            walk += random.choices(nodes, weights, k=1)
            # walk += random.sample(list(nx.neighbors(self.graph, cur_node)), 1)  # todo 添加按权重游走的采样方式
        return walk

    def do_walks(self, length):
        for start_node in range(self.node_size):
            yield self.small_walk(start_node, length)

    def sample(self, length, n_repeat):
        """
        从随机游走的结果中截取结果返回，每个节点轮流作为起始节点
        :param length: 游走长度
        :param n_repeat: 游走次数
        :return: （start_id,end_id,edge_type)
        """
        if not self.window:
            raise ValueError("window not set")

        if not self._path2id:
            self._path2id = {}
            path_id = 0
            for w in range(1, self._window + 1):
                for i in product(self._node_types, repeat=w + 1):
                    self._path2id[i] = path_id
                    path_id += 1

            self._path_size = len(self._path2id)
            self._id2node = {v: k for k, v in self._node2id.items()}
            self._id2path = {v: k for k, v in self._path2id.items()}

        samples = []

        for repeat in range(n_repeat):
            for walk in self.do_walks(length):
                cur_len = 0
                for i, node_id in enumerate(walk):
                    cur_len = min(cur_len + 1, self._window + 1)  # 当window=n的时候，最长路径有n+1个节点
                    if cur_len >= 2:
                        for path_length in range(1, cur_len):
                            sample = (walk[i - path_length], walk[i],
                                      self._path2id[tuple([self._id2type[t] for t in walk[i - path_length:i + 1]])])
                            # print(tuple([self.id2type[t] for t in walk[i-path_length:i + 1]]))
                            samples.append(sample)
        return samples

    def print_statistics(self):
        print(f'size = {self.node_size}')

def getNeighbors(edge_type_count, num_nodes, edge_data_by_type, edge_types, num_neighbor_samples):
    neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]
    for r in range(edge_type_count):
        g = edge_data_by_type[edge_types[r]]
        for (x, y) in g:
            x = int(x)
            y = int(y)
            neighbors[x][r].append(y)
            neighbors[y][r].append(x)
        for i in range(num_nodes):
            if len(neighbors[i][r]) == 0:
                neighbors[i][r] = [i] * num_neighbor_samples
            elif len(neighbors[i][r]) < num_neighbor_samples:
                neighbors[i][r].extend(
                    list(np.random.choice(neighbors[i][r], size=num_neighbor_samples - len(neighbors[i][r]))))
            elif len(neighbors[i][r]) > num_neighbor_samples:
                neighbors[i][r] = list(np.random.choice(neighbors[i][r], size=num_neighbor_samples))

    return neighbors
# def generatePairs(all_walks, window):
#     pairs = []
#     skip_window = window // 2
#
#
# def getNeighborNode():

def load_a_HIN_from_pandas(edges, print_graph=False):
    """
    单向边：edges = list(pd.df)
    """

    def reverse(df):
        """
        reverse source & dest
        """
        df = df.rename({'source_node': 'dest_node', 'dest_node': 'source_node',
                        'source_class': 'dest_class', 'dest_class': 'source_class'},
                       axis=1)
        # reverse edge_class
        df.edge_class = df.edge_class.map(lambda x: '-'.join(reversed(x.split('-'))))
        return df

    print('load graph from edges...')
    g = HIN()
    if isinstance(edges, list):
        edges = pd.concat(edges, sort=False)
    edges = edges.append(reverse(edges), sort=False, ignore_index=True)

    for index, row in edges.iterrows():
        g.add_edge(row['source_node'], row['source_class'],
                   row['dest_node'], row['dest_class'], row['edge_class'],
                   row['weight'])
    if print_graph:
        g.print_statistics()
    print('finish loading graph!')
    return g

def load_a_HIN_from_pandas_neighbors(edges, print_graph=False):
    """
    单向边：edges = list(pd.df)
    """

    def reverse(df):
        """
        reverse source & dest
        """
        df = df.rename({'source_node': 'dest_node', 'dest_node': 'source_node',
                        'source_class': 'dest_class', 'dest_class': 'source_class'},
                       axis=1)
        # reverse edge_class
        df.edge_class = df.edge_class.map(lambda x: '-'.join(reversed(x.split('-'))))
        return df

    print('load graph from edges...')
    g = HIN()
    if isinstance(edges, list):
        edges = pd.concat(edges, sort=False)
    edges = edges.append(reverse(edges), sort=False, ignore_index=True)

    edge_data_by_type = dict()

    list_source_node = list(edges['source_node'])
    list_dest_node = list(edges['dest_node'])
    list_edge_class = list(edges['edge_class'])
    edges_ = list(zip(list_source_node, list_dest_node))
    for i in range(len(list_edge_class)):
        if list_edge_class[i] not in edge_data_by_type:
            edge_data_by_type[list_edge_class[i]] = [edges_[i]]
        else:
            edge_data_by_type[list_edge_class[i]].append(edges_[i])

    edge_types = list(set(list_edge_class))

    for index, row in edges.iterrows():
        g.add_edge(row['source_node'], row['source_class'],
                   row['dest_node'], row['dest_class'], row['edge_class'],
                   row['weight'])
    if print_graph:
        g.print_statistics()
    print('finish loading graph!')
    return g, edge_data_by_type, edge_types


# if __name__ == '__main__':
#     hin = HIN()
#     hin.window = 4
#     # hin.window = 6
#     # hin.add_edge('A', 'Dr', 'a', 'Di', None, 0.3)
#     # hin.add_edge('B', 'Dr', 'b', 'Di', None, 0.3)
#     # hin.add_edge('C', 'Dr', 'c', 'Di', None, 0.3)
#     # hin.add_edge('A', 'Dr', 'b', 'Di', None, 0.3)
#     # hin.add_edge('C', 'Dr', 'b', 'Di', None, 0.3)
#     # hin.add_edge('c', 'Di', 'A', 'Dr', None, 0.3)
#     # hin.add_edge('a', 'Di', 'B', 'Dr', None, 0.3)
#     # hin.add_edge('A', 'Dr', 'B', 'Dr', None, 0.3)
#
#     hin.add_edge('A', 'Dr', 'B', 'Di', None, 0.3)
#     hin.add_edge('B', 'Di', 'C', 'Dr', None, 0.3)
#     hin.add_edge('C', 'Dr', 'D', 'Di', None, 0.3)
#     hin.add_edge('D', 'Di', 'E', 'Dr', None, 0.3)
#     hin.add_edge('E', 'Dr', 'F', 'Di', None, 0.3)
#     hin.add_edge('F', 'Di', 'A', 'Dr', None, 0.3)
#
#     print(hin.small_walk(hin._node2id['A'], 4))
#     print(hin.sample(3))
#     print(hin.node_size)
#     print(hin._path_size)
#
#     print(hin.graph.edges)


------------------------------------------
import torch
import numpy as np
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
# import torchsnooper

class HIN2vec(nn.Module):
    def __init__(self, edge_types, neighbors, node_size, num_sample_nei, path_size, embed_dim, sigmoid_reg=True, r=False):
        super().__init__()
        def binary_reg(x: torch.Tensor):
            raise NotImplementedError()

        self.reg = torch.sigmoid if sigmoid_reg else binary_reg
        self.__initialize_model(node_size, path_size, embed_dim, r)
        self.neighbors = neighbors
        self.edge_types = edge_types
        self.trans1 = nn.Linear(embed_dim, embed_dim)
        self.trans2 = nn.Linear(embed_dim * num_sample_nei, embed_dim)

    def __initialize_model(self, node_size, path_size, embed_dim, r):
        self.embeds_start = nn.Embedding(node_size, embed_dim)
        self.embeds_end = self.embeds_start if r else nn.Embedding(node_size, embed_dim)
        self.embeds_path = nn.Embedding(path_size, embed_dim)

    def getNeighborNodes(self, neighbors, nodes):
        '''
        :param neighbors: [node_1][edge_type] = [node2,....]
        :param nodes:
        :return:
        '''
        node_neighbors = {}
        for i in nodes:
            temp = []
            for n in neighbors[i][:]:
                temp.extend(n)
            node_neighbors[i] = temp

        return node_neighbors

#     @torchsnooper.snoop()
    def forward(self, neighbors, start_node, end_node, path):
        embeds_start_nodes = []
        for n in start_node:
            embeds_neighbors_all_type = []
            for e in range(len(self.edge_types)):
                node_neighbors_by_type = torch.tensor(neighbors[n][e]).cuda()
                embeds_neighbors_by_type = self.embeds_start(node_neighbors_by_type)
                # aggregator
                embeds_neighbors_by_type_agg = self.trans1(torch.mean(embeds_neighbors_by_type, dim=0))
                embeds_neighbors_all_type.append(embeds_neighbors_by_type_agg)
            embeds_start_nodes.append(torch.cat([i for i in embeds_neighbors_all_type]))
        embeds_start_nodes = torch.stack(embeds_start_nodes, dim=0)
        # embeds_start_nodes = self.embeds_start(start_node)
        embeds_start_nodes = self.trans2(embeds_start_nodes)
        embeds_end_ndoes = self.embeds_end(end_node)
        embeds_path = self.embeds_path(path)
        embeds_path = self.reg(embeds_path)

        agg = torch.mul(embeds_start_nodes, embeds_end_ndoes)
        agg = torch.mul(agg, embeds_path)

        output = torch.sigmoid(torch.sum(agg, dim=1))

        return output

    # def forward(self, neighbors, start_node, end_node, path):
    #     node_neighbors = self.getNeighborNodes(neighbors, start_node)
    #     embeds_start_nodes = []
    #     for node, node_neigh in node_neighbors.items():
    #         node_neigh = torch.tensor(node_neigh)
    #         embeds_start_nodes.append(torch.cat([i for i in self.embeds_start(node_neigh)]))
    #     embeds_start_nodes = torch.stack(embeds_start_nodes, dim=0)
    #     # embeds_start_nodes = self.embeds_start(start_node)
    #     embeds_start_nodes = self.trans(embeds_start_nodes)
    #     embeds_end_ndoes = self.embeds_end(end_node)
    #     embeds_path = self.embeds_path(path)
    #     embeds_path = self.reg(embeds_path)
    #
    #     agg = torch.mul(embeds_start_nodes, embeds_end_ndoes)
    #     agg = torch.mul(agg, embeds_path)
    #
    #     output = torch.sigmoid(torch.sum(agg, axis=1))
    #
    #     return output

def train(neighbors, log_interval, model, device, train_loader: DataLoader, optimizer, loss_function, epoch):
    model.train()
    for idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(neighbors, data[:, 0], data[:, 1], data[:, 2])
        loss = loss_function(output.view(-1), target)
        loss.backward()
        optimizer.step()

        if idx % log_interval == 0:
            print(f'\rTrain Epoch: {epoch} '
                  f'[{idx * len(data)}/{len(train_loader.dataset)} ({100. * idx / len(train_loader):.3f}%)]\t'
                  f'Loss: {loss.item():.3f}\t\t',
                  # f'data = {data}\t target = {target}',
                  end='')

class NSTrainSet(Dataset):
    """
    完全随机的负采样 todo 改进一下？
    """

    def __init__(self, sample, node_size, neg=5):
        """
        :param node_size: 节点数目
        :param neg: 负采样数目
        :param sample: HIN.sample()返回值，(start_node, end_node, path_id)
        """

        print('init training dataset...')

        l = len(sample)

        x = np.tile(sample, (neg + 1, 1))
        y = np.zeros(l * (1 + neg))
        y[:l] = 1

        # x[l:, 2] = np.random.randint(0, path_size - 1, (l * neg,))
        x[l:, 1] = np.random.randint(0, node_size - 1, (l * neg,))

        self.x = torch.LongTensor(x)
        self.y = torch.FloatTensor(y)
        self.length = len(x)

        print('finished')

    def __getitem__(self, index):
        return self.x[index], self.y[index]

    def __len__(self):
        return self.length
------------------------
import torch
import pandas as pd
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# from walker import load_a_HIN_from_pandas_neighbors, getNeighbors
# from model_merge import NSTrainSet, HIN2vec, train

# set method parameters
window = 5
walk = 5
walk_length = 55
embed_size = 200
neg = 5
num_sample_nei = 3
sigmoid_reg = True
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f'device = {device}')

# set dataset [PLEASE USE YOUR OWN DATASET TO REPLACE THIS]
demo_edge = pd.read_csv('./train_tcm.csv', index_col=0)

edges = [demo_edge]

print('finish loading edges')

# init HIN
hin, edge_data_by_type, edge_types = load_a_HIN_from_pandas_neighbors(edges)
hin.window = window
dataset = NSTrainSet(hin.sample(walk_length, walk), hin.node_size, neg=neg)
neighbors = getNeighbors(
    edge_type_count=len(edge_types),
    num_nodes=hin.node_size,
    edge_data_by_type=edge_data_by_type,
    edge_types=edge_types,
    num_neighbor_samples=num_sample_nei)

hin2vec = HIN2vec(
    edge_types,
    neighbors,
    hin.node_size,
    num_sample_nei,
    hin.path_size,
    embed_size,
    sigmoid_reg)

# load model
# hin2vec.load_state_dict(torch.load('hin2vec.pt'))

# set training parameters
n_epoch = 1
batch_size = 64
log_interval = 200
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

if torch.cuda.is_available():
    print('Use ', device)
    hin2vec = hin2vec.to(device)
else:
    print('Use CPU')


optimizer = optim.Adam(hin2vec.parameters())  # 原作者使用的是SGD？ 这里使用AdamW
loss_function = nn.BCELoss()


for epoch in range(n_epoch):
    train(neighbors, log_interval, hin2vec, device, data_loader, optimizer, loss_function, epoch)

torch.save(hin2vec, 'hin2vec.pt')

# set output parameters [the output file is a bit different from the original code.]
node_vec_fname = 'node_vec_merge_' + \
                 str(window) + '_' +\
                str(walk) + '_' +\
                str(walk_length) + '_' +\
                str(embed_size) + '_' +\
                str(neg) + '_' +\
                str(num_sample_nei) + '_' +\
                str(sigmoid_reg) + '_' +\
                str(n_epoch) + '_' +\
                str(batch_size) + '_' +\
                str(log_interval) +\
                 '.txt'
# path_vec_fname = 'meta_path_vec.txt'
path_vec_fname = None

print(f'saving node embedding vectors to {node_vec_fname}...')
node_embeds = pd.DataFrame(hin2vec.embeds_start.weight.data.cpu().numpy())
node_embeds.rename(hin.id2node).to_csv(node_vec_fname, sep=' ')

if path_vec_fname:
    print(f'saving meta path embedding vectors to {path_vec_fname}...')
    path_embeds = pd.DataFrame(hin2vec.embeds_path.weight.data.numpy())
    path_embeds.rename(hin.id2path).to_csv(path_vec_fname, sep=' ')

# save model
# torch.save(hin2vec.state_dict(), 'hin2vec.pt')